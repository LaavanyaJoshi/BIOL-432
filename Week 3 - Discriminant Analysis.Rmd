---
title: "Week 3 - Discrmininant Analysis"
output:
  html_document:
    df_print: paged
---

#### Last week was about using PCA to identify uncorrelated axes of variation, seeing how groups post-hoc mapped onto the major PC axes
#### This week is about Regularized Discrminant Analsysis as an extension of PCA that incorporated a categorical varaible for SUPERVISED LEARNING (you know the groups aheda of time and find a model to distinguish them). With RDA, the 1st PC axis is adjusted by axis loadings, then a linear or nonlinear wquation to predict the repsonse variable is applied. There is ay least one binary response variable and many continuous features (a variable can be redefined with N categories using N-1 binary variables).

### Discmrinant Analysis (DA) = Linear Discrminant Analysis (LDA) = Discrminant Function Analysis (FDA)
### LDA is a generalization of Fisher's Linear Discrminant Function or Quadratic Discrminant Analysis (QDA).
### QDA allows for nonlinear predictors by including a turning parameter for unequal variances in the features predictions.
### Both LDA and QDA were eventually generalized to Regularized Discmrinant Analysis (RDA) by adding a second tuning parameter, represented by sigma and gamma.


```{r}
library(ggplot2)
library(dplyr)
library(MASS)

## dplyr and MASS have some common functions, so you may have to type package name::function()

source("http://bit.ly/theme_pub")
theme_set(theme_pub())

Virus <- read.csv("https://colauttilab.github.io/Data/ViralMetData.csv", header=T)
```


#### This week's data is about the nasal metabolome, where metabolic profiles of patients are studied with an LDA to categorize patients with a viral infection (96% accuracy) and distinguish COVID cases from infleunza and RSV (85% accuracy).
### Metabolomics = analysis of many chem profiles

```{r}
names(Virus)
str(Virus)
```

#### Sample.Name – A unique identifier for each sample
#### Batch.Number – A unique number for each ‘batch’. All the samples with thesame ‘batch’ were run on the same equipment at the same time. 
#### Class.name – This is the group classifier, and there are five groups corre-sponding to the type of infection or control
#### VTM – this is just the liquid used to stabilize the nasal swab. It is purchasedfrom a biotech company so the exact chemical profile is unknown, but includ-ing it in the analysis acts as one type of control
#### Control – nasal swabs from patients with no known infection
#### COVID19 – patients who tested positive for COVID-19 via qPCR
#### Influenza – patients who tested positive for Influenza via qPCR
#### RSV – patients who tested positive for Respiratory Syncytial Virus (RSV) viaqPCR
#### Age, Sex – Age and sex of the patient
#### Ct is short for ‘count’ or ‘count threshold’ and it a measure of viral loadin qPCR; the number of PCR cycles rune before target sequnece reaches detection threshold; higher CT = lower viral load (qPCR = real-time PCR but DOES NOT EQUAL reverse-transcription PCR)
#### The other columns each show the relative concentration of a specific metabolite/chemical in the metabolome.


```{r}
ggplot(aes(x = Pyruvic.acid), data = Virus) + 
  geom_histogram(bins = 30)
```


#### This is a good set of data for log-normal transformation with many values below 10 and a few values more than 90. This will bring values closer together so the distribution looks more normal.

```{r}
ggplot(aes(x = log(Pyruvic.acid+1)), data = Virus) + 
  geom_histogram(bins = 30)
```



```{r}
### Quality checks and modifications to fit assumptions of multivariate normality

Virus$Batch.Number <- as.factor(Virus$Batch.Number) #set variable as a factor

## The following won't work: Response <- Virus %>% select(1:6) - it won't separate response variables / FEATURES from predicting traits
```

### YOU NEED TO SPECIFY SELECT FOR dplyr

```{r}
Response <- Virus %>%
  dplyr::select(1:6)
Features <- Virus %>%
  dplyr::select(-c(1:6))
```


#### To verify that the correct columns are subset in each dataset, use head(), str(), and names()

```{r}
head(Response)
str(Response)
names(Response)

head(Features)
str(Features)
names(Features)
```


```{r}
Scaled <- Features %>%
  mutate_all(scale) # regular scaling
```

```{r}
Scaled %>%
  select_if(function(x) any(is.na(x))) %>%
  names() # this only CHECKS for missing data
```

#### Since everything has been scaled to a mean of 0 with 7 columns with missing data, dplyr may be ised to replace missing data with 0.

```{r}
ScalComp <- Scaled %>%
  mutate(Putrescine = ifelse(is.na(Putrescine), 0, Putrescine),
         Leu = ifelse(is.na(Leu), 0, Leu),
         Asp = ifelse(is.na(Asp), 0, Asp),
         Lactic.acid = ifelse(is.na(Lactic.acid), 0, Lactic.acid),
         Butyric.acid = ifelse(is.na(Butyric.acid), 0, Butyric.acid),
         Succinic.acid = ifelse(is.na(Succinic.acid), 0, Succinic.acid),
         Pyruvic.acid = ifelse(is.na(Pyruvic.acid), 0, Pyruvic.acid))
```


#### Now, check the QA/QC output

```{r}
mean(ScalComp$Gly)

sd(ScalComp$Gly)

ggplot(aes(x = Gly), data = ScalComp) + 
  geom_histogram(bins = 30)
```

#### Data has a mean close enough to 0 and an sd to 1 that it can be used - it;s usually not perfect.


```{r}
dim(ScalComp) #shows the dimensions of sclaed features
```

#### Ther are almost as many features (k = 124) as rows of observations (n = 221).This means putting this info into an RDA could overfit the model (similar to false discovery problem if a different linear model for each feature is used as a predictor). 
#### Therefore, use the PCA to reduce dimensionality of the data. Run a PCA and keep only major axes. Variables have LAREADY BEEN SCALED, so we DON'T NEED cor = T

```{r}
PCA <- princomp(ScalComp, cor = F)
summary(PCA)
```

#### NOTE that the first 9 axes give 80% of the variation in metabolite profiles: the dataset issimplified but it's not perfect
#### First, 10 PC axes is a lot to visualize. Second, most of the variation may be due to patient differences, NOT the respiratory infections., meaning the first 10 axes may not capture the divergence.
#### These problems are addressed by DA. Re-scale the PC axes to find component aces that best distinguish patient with different infections, but reduce the number of axes to avoid spurious correlations. Here, exclude metabolites that are unlikely to distinguish amongst infection status.
#### With a categorical response variable, the categories can screen for features more likely to distinguish patients with different viruses. Look at the predictive power of each feature, and keep only those that are "reasonably good," a criteria that changes for #features, model, etc.
#### You could use a simple linear model for each features, but this would warrant making 124 models. Therefore, convert data to long format.

```{r}
library(tidyr)

LongVirus <- ScalComp %>%
  mutate(Class.name = Virus$Class.name) %>% #Add response variable, creating the linear model
  pivot_longer(cols = -Class.name,
               names_to = "Chem", #Does the feature name
               values_to = "Conc") #Does the observed value

str(LongVirus)
```

#### The original 221 rows were repeated for each of the 124 features, making 27404 rows

```{r}
LongVirus %>% 
  group_by(Chem) %>%
  summarize(MeanConc = mean(Conc),
            sd = sd(Conc),
            max = max(Conc),
            min = min(Conc))
```

#### We need to run separate linear models and also extract their p-values. The ANOVA function gives the p-values but also an outputwhich is an object and can thus be subsetted into a new data frame or vector. This may be used in a for loop 

```{r}
anova(lm(ScalComp$Gly ~ Response$Class.name))[1,"Pr(>F)"]

PVals <- LongVirus %>% 
  group_by(Chem) %>% 
  summarize(P = anova(lm(Conc ~ Class.name))[1,"Pr(>F)"]) %>%
  dplyr::select(Chem,P)
```

#### Check the first few rows to see if it works

```{r}
head(PVals)
```

#### When selecting a cut-off value, consider: a smaller p-valuegives a more strict cut-off and a stronger prediction, but less features will be retained. First, plot a histogram.

```{r}
ggplot(aes(x = P), data = PVals) + 
  geom_histogram(bins = 30)
```

#### About 40 features are close to 0, meaning many of the metabolites - OR FEATURES - differ amongst groups. However, these 40 features, insetad of 124, is still a lot for 221 observations. 
#### Consider the biology and goal of this model. Use DA to find chem signatures that distinguish patient or control samples from these 5 groups.

```{r}
unique(Virus$Class.name)
```

### To understand DA, think in terms of binary cetgories.
#### Consider everything as a separate but related goal, such as control stabilizing solution (VTM) from patient samples (all others), distinguish COVID from healthy patients, and distinguish COVID from other illnesses.

```{r}
PCOV <- LongVirus %>%
  filter(Class.name %in% c("COVID19", "Influenza", "RSV")) %>%
  mutate(NewGroup = replace(Class.name, Class.name == "Influenza", "NonCOV")) %>%
  mutate(NewGroup = replace(NewGroup, Class.name == "RSV", "NonCOV")) %>%
  group_by(Chem) %>%
  summarize(P = anova(lm(Conc ~ NewGroup))[1,"Pr(>F)"]) %>%
  dplyr::select(Chem,P)

ggplot(aes(x = P), data = PCOV) + 
  geom_histogram(bins = 30) + 
  xlim(0,0.1)
```

#### The result is 14 fetaures with p < 0.05, so defin a new features dataset.

```{r}
Keep <- PCOV %>%
  filter(PCOV$P < 0.05)

Keep <- paste(Keep$Chem) # gives a vector of chem names to select columns

ScaledSub <- ScalComp %>%
  dplyr::select(all_of(Keep))
names(ScaledSub)
```

### With the subset of features, the LDA can be run. the lda() function is from the MASS package - 'equivalent' to princomp for PCA.
### To specify the model, use Y ~ . format, where Y is the categorical variable and . means "all other columns of data.. Specify the data object with the data parameter


```{r}
RDA <- Response %>%
  mutate(NewGroup = replace(Class.name, Class.name != "COVID19", "NonCOV")) # Since there are responses and features in 2 different objects, you don't need the data = portion

LDA <- lda(x = ScaledSub, grouping = RDA$NewGroup) #run the model
```


### NOTE how more time was spent processing the data than actually running the model, which was just one line of code. This represents data science in the real world (Quality Assurance and Quality Control)

```{r}
str(LDA)

summary(LDA)
```

### Unlike lm(), the summary() function for an lda object summarizes the object itself, no the model. Left column is the names of the list items and "Length" gives number of elements.

```{r}
LDA$counts
```

### Scaling shows the factor loadings, aka LD eigenvectors.
#### COMPARE with PCA eigenvectors: PCA would give 14 eigenvectors, while one LD eigenvector is obtained for the 14 features. Both eigenvector types are scaled vectors with loadings for each of the 14 features. However, the number of axes in an LDA are given by # categories of the response variable, NOT number of features. For instance, running the LDA on the 5 groups of class.name (which has 5 types of infection statuses), the result is 4 LDA axes, beause you need at least 4 binary variables to distinguish the 5.
#### The eigenvector loadings show how each feature contributes to the LD1 aces, like PC eigenvector loadings, but they're scaled to differentiate between the 2 groups, while PCA is for all features.

### PCA: # of eigenvectors = # of PC axes = # of features
### LDA = # of eigenvectors = # of LDA axes = # of Group Categories - 1

```{r}
LDA$scaling
```

#### Higher values of LD1 are determined mostly by higher values of C2, Carnosine, Tyrosine, and lower values of Betain, C16, Taurine, LYSOC18. INterpreting these loadings and researching the metabolites contributes to the advancement of biological knowledge.

### LDA output does not give scores - you have to make predictions from data.

```{r}
Pred <- predict(LDA)
summary(Pred)
```

### The class and x predictions have the same length as the # observations as dataset.

### x object = predicted score for LDA axis

```{r}
head(Pred$x)
```

### Class object = predicted category

```{r}
head(Pred$class)
```

#### Vectors have the same length because LD1 score predicts the category (COVID or not) for every individual.


### Use a confusion matrix to check the accuracy; this can indicate the accuracy, specificity, and sensitivity of the LDA model.

```{r}
CatDat <- data.frame(Observed = as.factor(RDA$NewGroup),
                     Predicted = Pred$class)
table(CatDat)
```

### The posterior probability is a concept in Bayesian stats, relating to the assigning of each observation to each group, measuring the confidence of the model. Values closer to 0 or 100 % are of higher confidence belonging to one group or another.

```{r}
Post <- data.frame(Pred$posterior)
head(Post)
```

#### In all 6 cases, there is a more than 95% probability that the individual belongs ot the NonCOV group. The confidence of the model predictions can be viewed by plotting the posterior probabilities.

```{r}
Post$Group <- RDA$NewGroup
ggplot(aes(x = COVID19, fill = Group), data = Post) + 
  geom_histogram(bons = 30, position = "dodge") + 
  facet_wrap(vars(Group))
```

#### This model does better at predicting non-COVID cases than COVID cases

#### The x-axis is the predicted probability the patient has COVID. Cmompare this to the LD1 scores.

```{r}
Post$LD1 <- as.vector(Pred$x)
ggplot(aes(x = LD1, y = COVID19, colour = Group), data = Post) + 
  geom_point(shape = 21, alpha = 0.3)
```

#### The porbability is a nonlinear function of LD1. A new patient may look at their metabolite profile to predict if they have COVID; policy is usually based on a firm choice. 
#### When asking which value of LD1 should categorize the pateint, one answer is the point along LD corresponding to 0.5 on the y-axis. Alternatively, one may want to error on the side of caution to limit the false-negative rate

### The different threshold values of LD1 on the number of false positives VS false negatives is shown in the Receiver-Operator Curve graph.

### The ROC is a measure of model performance based on rates of false positives VS negatives. To find the cruve, set a value of LD1, look at the confusion matrix, calculate sensitivity VS specificity at a given value of LD1. Repeat for different values of LD1, and graph the results.

```{r}
library(pROC)

plot.roc(Group~LD1, data = Post)
```

#### On the left side, it starts with 100% specificity and 0% sensitivity. After specificity = 0.8, much specificity is sacrificed for small increase in sensitivity.

### The ROC may have a True Positive Rate (TPR) on y-axis and False Positive Rate (FPR) on x-axis, ranging from 0-1. These graphs are equivalent bevaise TPR = sensitivity and FPR = specificity.
### The curve for a 100% accurate model would be a straight vertical line with 100% of both.

### One can measure the Area Under the ROC (AUROC or AUC) which is a single value that quantifies the performance of the model, shown by the ROC.The random model is a simple triangle with a min AUC = 0.5. As model improves in accuracy, one appraoches a square with 100% accuracy across all values of specificity, so max value of 1.

```{r}
auc(Group~LD1, data = Post) #also from the pROC package to get the area
```

#### The model can thus predict if a pateint has COVID with 93% accuracy. This isn't bad, but it's inflated by an unbalanced sample size.


### To expand the model to look at all different classes, set binary axes and run a separate LDA for each.

```{r}
LDAMulti <- lda(x = ScalComp, grouping = RDA$Class.name)
summary(LDAMulti)
```

#### Now there are 5 categories and 4 LD axes, each being like a mini-LDA for each category VS all others grouped together.

### Like for PCA, a score can be calculated for each observation by multiplying standardized features values by its factor loading.
### The QDA is a nonlinear RDA if groups have different variances and means. It scales transformation matrix that calculates LD axes. "Quadratic" is the predictor, unlike linear LD1 axis in LDA. 
#### Recall we grouped VTM and uninfected with Influenza and RSV, which are biologically heterogenous and more variable than COVID.

```{r}
QDA <- qda(x = ScaledSub, grouping = RDA$NewGroup)
summary(QDA)

QDApred <- predict(QDA)
summary(QDApred)
```

### However, this summary lacks the linear predictor; predictions become nonlinear, meaning an ROC or AUROC.

```{r}
QDAcon <- data.frame(Obs = as.factor(RDA$NewGroup), Pred = QDApred$class)
table(QDAcon) #new confusion matrix for predictions
```

### Comparing the two shows the trade-off between prediction and interpretability. QDA is better at predicting the patient category but LDA is more interpretable. So, this matrix performs better than the LDA.

### The Regularized Discrminant Analysis (RDA), a generalization that includes LDA and QDA. It requires the tuning parameters sigma and gamma.
### The lambda value is between 0 and 1. 0 is closer to QDA and 1 is closer to LDA. Gamma tunes everything towarda the average variance of the features. This results in a range of mdoels with 4 special cases.
### LDA results when gamma = 0 and lambda = 1, using a pooled covariance estimate
### QDA results when gamma = lambda = 0, using individual covariances for each group.
### When gamma = 0, lambda defines how much the individual groups should have separate covariance structures. For PCA, a single covariance matrix is used for all samples, scaled to a correlation matrix, equaivalent to a covariance matrix of z-scores.
### This gets refined during ML appraoches
### UNLIKE LDA, RDA can be applied to data with more features than observations, because it uses a regularized covariance matrix, not a covariance matrix like LDA and PCA.
### When choosing values for lambda and gamma, ML allows for cross-validation to find values.

```{r}
library(klaR)
library(caret)
library(lattice)
```

### One way to optimize the tuning parameters is to set a range of values for each, run a model wth each combination of values, then compare the model fits = GRID SEARCH
### However, grid search warrants a number of models that increases exponentially with the number of parameters and degree of precision needed in parameters, as grid lines get smaller or # features increases. Therefore, it's mainly used for simpler models.
### Often, a good model will do, even if it's not the best model.
### A random search may be used - randomly selecting values along a range of 0 to 1 for each parameter, running the model, then measuring performance

```{r}
CTL <- trainControl(method = "LOOCV", #sampling method - LOOCV is used because there aren't that many observations
                    classProbs = T, #specifies we need class probabilities specific to each subsample
                    verboseIter = F, #avoids giving more detailed feedback
                    search = "random") #specifies a random earch; an alternate would be grid

str(CTL)
```

### If the train function takes too long, reduce tuneLength to a smaller number of 3/4

```{r}
set.seed(432)
randomRDA <- train(x = ScaledSub, y = RDA$NewGroup,
                   method = "rda",
                   metric = "Accuracy",
                   tuneLength = 24,
                   trControl = CTL)
randomRDA
```

### WHY DOES THE ABOVE TAKE SO LONG?? We're using random values to tune parameters, LOOCV is used on observations, and then this is repeated for new values. Therefore, only 24 parameter combinations are tested.
#### The results show a list of values for different combinations of values for gamma and lambda, with accuracy estimated suing cross-validation.


### The kappa column is an adjustment of the accuracy - important for ML and assessing model accuracy. It goes from -1 to 1. 0 means the model is no better than chance and 1 means perfect accuracy. Negative values mean the model performs less than chance.
#### The kappa metric addresses the issue of uneven samples sizes for COVID vs non-COVID
### Cohen's k measure inter-rater reliability; more robust to random sampling of imbalanced categories compared to percet accuracy but harder to interpret

### Convert the table to a graph for visualization and interpretation. Graph gamma and lambda as x and y axes, then use accuracy or kappa as a sclaing vector for size or colour. 

```{r}
ggplot(randomRDA) + 
  theme(legend.position = "bottom")
```

#### Size correpsonds to model accuracy.

```{r}
ggplot(data = randomRDA$results, aes(x = gamma, y = lambda, size = Kappa)) + 
  geom_point() + 
  theme(legend.position = "bottom")
```

#### kappa is higher if gamma is lower and lambda is higher. Setting gamma at 0 and lambda at 1 just creates the LDA, which is more interpretable and thus ideal.


### Adjusting the tuning parameters will yield something other than a QDA or LDA
```{r}
RDAmod <- rda(x = ScaledSub, grouping = RDA$NewGroup,
              regularization = c(gamma = 0.5, lambda = 0.9))
summary(RDAmod)
```

### This yields more list items than QDA, including estimated error rate, individual group covariance, and pooled covariances


### RECALL the issue of overfitting. For RDA, this was avoided with cross-validation. Here, we might aplit data into training and validation sets, but an ideal way to do this is to select every Nth row. For instance, divide row number by 2.
```{r}
Rows <- c(1:nrow(RDA))
Train <- Rows %% 2 == 1 # Odd numbers have a remainder of 1 and go into train object
Validate <- Rows %% 2 == 0

head(RDA[Train,])

head(RDA[Validate,])
```

### For more complex models, use a larger number of observations and divide by a larger number (ex. 4)

### Now, run code on Train, and mak predictions for Validate

```{r}
set.seed(432)
randomRDA2 <- train(x = ScaledSub[Train,], y = RDA$NewGroup[Train], # specifying specific rows that correspond to Train; first set has a comma because input object is a data matrix, but the object is a vector in the second set
                    method = "rda",
                    metric = "Accuracy",
                    tuneLength = 24,
                    trControl = CTL)
randomRDA2
```

```{r}
ggplot(randomRDA2) + 
  theme(legend.position = "bottom")
```

### Finally, run the model on the training dataset

```{r}
RDAmod2 <- rda(x = ScaledSub[Train,], grouping = RDA$NewGroup[Train],
               regularization = c(gamma = 0.6, lambda = 0.25))
```


### To validate the model, generate the confusion matrix for Validate
```{r}
Pred <- predict(RDAmod2, ScaledSub[Validate,])
str(Pred)
```

```{r}
CatDat <- data.frame(Obs = as.factor(RDA$NewGroup[Validate]),
                     Pred = Pred$class)
table(CatDat)
```

```{r}

```

### All these methods of cross-validation have parallels with lmer(), and these are all part of a general modelling problem called parameter estimation. Other examples include Bayesian optimization, genetic algorithms, gradient descent, etc.









